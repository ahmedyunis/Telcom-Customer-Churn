# -*- coding: utf-8 -*-
"""Telecom_Churn_Kaggle (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y5r78m7wQzKfX-jXR8jdP-ZjQZIKo1yQ
"""

# Commented out IPython magic to ensure Python compatibility.
import  numpy as np 
import pandas as pd
import pandas_profiling as pdf 
import matplotlib.pyplot as plt  
import  seaborn as  sns 
# %matplotlib inline
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

Data_df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')

Data_df.info

Data_df.head()

Data_df.isnull().sum()

Data_df.dtypes

Data_df.TotalCharges = pd.to_numeric(Data_df.TotalCharges, errors='coerce')
Data_df.isnull().sum()

pdf.ProfileReport(Data_df)

Data_df.dropna(inplace = True)
Data_df.isnull().sum()

replace_cols = [ 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
                'TechSupport','StreamingTV', 'StreamingMovies']
for i in replace_cols : 
    Data_df[i]  = Data_df[i].replace({'No internet service' : 'No'})

pdf.ProfileReport(Data_df)

Churn_data = Data_df[Data_df['Churn'] == 'Yes']
NotChurn_data =Data_df[Data_df['Churn'] == 'No']

plt.figure(figsize=(6,6))
pl = sns.countplot(x = 'gender', data =Data_df)
plt.title('Gender Distribution of All Data')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(Data_df)*100), (p.get_x()+0.1, p.get_height()+50))

plt.figure(figsize=(15,10))
plt.subplot(121)
pl= Churn_data.gender.value_counts().plot.bar(title = 'Gender Distribution of  ChurnData')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(Churn_data)*100), (p.get_x()+0.1, p.get_height()+35))

plt.subplot(122)
pl = NotChurn_data.gender.value_counts().plot.bar(title = 'Gender Distribution of NotChurnData')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(NotChurn_data)*100), (p.get_x()+0.1, p.get_height()+50))

plt.figure(figsize=(6,6))
plt.title('SeniorCitizen Distribution of all Data ')
plt.pie(Data_df.SeniorCitizen.value_counts(), labels= ['Senior','NotSenior'], autopct='%1.1f%%',explode= (0.1,0) ,shadow=True)

plt.figure(figsize=(10,10))
plt.subplot(121)
plt.title('SeniorCitizen Distribution of  ChurnData')
plt.pie(Churn_data.SeniorCitizen.value_counts(), labels= ['Senior','NotSenior'], autopct='%1.1f%%',explode= (0.1,0) ,shadow=True)

plt.subplot(122)
plt.title('SeniorCitizen Distribution of  NotChurnData')
plt.pie(NotChurn_data.SeniorCitizen.value_counts(), labels= ['Senior','NotSenior'], autopct='%1.1f%%',explode= (0.1,0) ,shadow=True)

plt.figure(figsize=(6,6))
pl = sns.countplot(x = 'Partner', data =Data_df,)
plt.title('Partner Distribution')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(Data_df)*100), (p.get_x()+0.1, p.get_height()+50))

plt.figure(figsize=(12,10))
plt.subplot(121)
pl= Churn_data.Partner.value_counts().plot.bar(title = 'Partner Distribution of  ChurnData')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(Churn_data)*100), (p.get_x()+0.1, p.get_height()+40))

plt.subplot(122)
pl = NotChurn_data.Partner.value_counts().plot.bar(title = 'Partner Distribution of NotChurnData')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(NotChurn_data)*100), (p.get_x()+0.1, p.get_height()+50))

plt.figure(figsize=(6,6))
plt.title('Dependents Distribution')
plt.pie(Data_df.Dependents.value_counts(), labels= ['Has','Not'], autopct='%1.1f%%',explode= (0.1,0) ,shadow=True)

plt.figure(figsize=(10,10))
plt.subplot(121)
plt.title('Dependents Distribution of  ChurnData')
plt.pie(Churn_data.Dependents.value_counts(), labels= ['Senior','NotSenior'], autopct='%1.1f%%',explode= (0.1,0) ,shadow=True)

plt.subplot(122)
plt.title('Dependents Distribution of  NotChurnData')
plt.pie(NotChurn_data.Dependents.value_counts(), labels= ['Senior','NotSenior'], autopct='%1.1f%%',explode= (0.1,0) ,shadow=True)

plt.figure(figsize=(8,8))
plt.title('tenure distribution')
sns.distplot(Data_df.tenure)

plt.figure(figsize=(8,8))
plt.title('MonthlyCharges distribution')
sns.distplot(Data_df.MonthlyCharges)

plt.figure(figsize=(8,8))
plt.title('TotalCharges distribution')
sns.distplot(Data_df.TotalCharges)

plt.figure(figsize=(6,6))
pl = sns.countplot(x = 'PhoneService', data =Data_df,)
plt.title('PhoneService Distribution')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(Data_df)*100), (p.get_x()+0.1, p.get_height()+50))

plt.figure(figsize=(10,10))
plt.subplot(121)
pl= Churn_data.PhoneService.value_counts().plot.bar(title = 'PhoneService Distribution of  ChurnData')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(Churn_data)*100), (p.get_x()+0.1, p.get_height()+50))

plt.subplot(122)
pl = NotChurn_data.PhoneService.value_counts().plot.bar(title = 'PhoneService Distribution of NotChurnData')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(NotChurn_data)*100), (p.get_x()+0.1, p.get_height()+50))

plt.figure(figsize=(10,10))
plt.subplot(121)
pl= Churn_data.MultipleLines.value_counts().plot.bar(title = 'MultipleLines Distribution of  ChurnData')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(Churn_data)*100), (p.get_x()+0.1, p.get_height()+30))

plt.subplot(122)
pl = NotChurn_data.MultipleLines.value_counts().plot.bar(title = 'MultipleLines Distribution of NotChurnData')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(NotChurn_data)*100), (p.get_x()+0.1, p.get_height()+50))

plt.figure(figsize=(10,10))
plt.subplot(121)
pl= Churn_data.InternetService.value_counts().plot.bar(title = 'InternetService Distribution of  ChurnData')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(Churn_data)*100), (p.get_x()+0.1, p.get_height()+30))

plt.subplot(122)
pl = NotChurn_data.InternetService.value_counts().plot.bar(title = 'InternetService Distribution of NotChurnData')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(NotChurn_data)*100), (p.get_x()+0.1, p.get_height()+50))

plt.figure(figsize=(10,10))
plt.subplot(121)
pl= Churn_data.Contract.value_counts().plot.bar(title = 'Contract Distribution of  ChurnData')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(Churn_data)*100), (p.get_x()+0.1, p.get_height()+30))

plt.subplot(122)
pl = NotChurn_data.Contract.value_counts().plot.bar(title = 'Contract Distribution of NotChurnData')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(NotChurn_data)*100), (p.get_x()+0.1, p.get_height()+50))

def plot_count(x, fig):
    plt.subplot(6, 2, fig)
    plt.title(x+ ' VS ChurnData')
    pl = sns.countplot(Churn_data[x])
    for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(Churn_data)*100), (p.get_x()+0.1, p.get_height()+30))

    plt.subplot(6,2,fig+1)
    plt.title(x+ ' VS NotChurnData')
    pl = sns.countplot(NotChurn_data[x])
    for p in pl.patches:
      pl.annotate('{:.2f}%'.format(p.get_height()/len(NotChurn_data)*100), (p.get_x()+0.1, p.get_height()+30))

plt.figure(figsize=(10, 30))
plot_count('OnlineSecurity', 1)
plot_count('OnlineBackup', 3)
plot_count('DeviceProtection', 5)
plot_count('TechSupport', 7)
plot_count('StreamingTV', 9)
plot_count('StreamingMovies', 11)

plt.figure(figsize=(6,6))
pl = sns.countplot(x = 'Churn', data =Data_df,)
plt.title('Churn Distribution')
for p in pl.patches:
        pl.annotate('{:.2f}%'.format(p.get_height()/len(Data_df)*100), (p.get_x()+0.1, p.get_height()+50))
plt.show()

Data_df.dtypes

df = Data_df.iloc[:,1:]

df['Churn'].replace(to_replace='Yes', value=1, inplace=True)
df['Churn'].replace(to_replace='No',  value=0, inplace=True)
df.head()

Data = pd.get_dummies(df)

Data.head()

Data.corr()

plt.figure(figsize=(30, 20))
sns.heatmap(Data.corr(), annot=True)

Data.columns

dr_cols = [ 'TotalCharges','gender_Male','Partner_Yes','Dependents_Yes','PhoneService_Yes','MultipleLines_No phone service',
           'MultipleLines_Yes','InternetService_Fiber optic','InternetService_No','OnlineSecurity_Yes','OnlineBackup_Yes',
           'DeviceProtection_Yes','TechSupport_Yes','StreamingTV_Yes','StreamingMovies_Yes','PaperlessBilling_Yes']
Data_Edit = Data.drop(dr_cols , axis=1)

plt.figure(figsize=(30, 20))
sns.heatmap(Data_Edit.corr(), annot=True)

norm_Data = MinMaxScaler().fit_transform(Data_Edit)

norm_Data = pd.DataFrame(norm_Data, columns = Data_Edit.columns)

Churn_count = len(norm_Data[norm_Data['Churn'] == 1])
Churn_index = norm_Data[norm_Data['Churn'] == 1].index

NotChurn_index = norm_Data[norm_Data['Churn'] == 0].index
rand_NotChurn_index =np.random.choice(NotChurn_index, Churn_count)

balanced_index = np.concatenate([rand_NotChurn_index, Churn_index])

balanced_Data = norm_Data.iloc[balanced_index]

sns.countplot('Churn', data = balanced_Data)

train, test = train_test_split(balanced_Data, test_size = 0.3)

train_x = train.drop('Churn', axis=1)
train_y = train[['Churn']]

test_x = test.drop('Churn', axis=1)
test_y = test[['Churn']]

pca = PCA()
train_x = pca.fit_transform(train_x)
test_x = pca.transform(test_x)

train_x = pd.DataFrame(data=train_x )
 test_x = pd.DataFrame(data= test_x)

LGRmodel = LogisticRegression()
LGRmodel.fit(train_x, train_y)
pred_y = LGRmodel.predict(test_x)

print (metrics.accuracy_score(test_y, pred_y))
print(confusion_matrix(test_y, pred_y))
print(classification_report(test_y, pred_y, target_names=['Churn','NotChurn']))

SVMmodel = SVC(kernel='linear') 
SVMmodel.fit(train_x,train_y)
pred_y = SVMmodel.predict(test_x)

print (metrics.accuracy_score(test_y, pred_y))
print(confusion_matrix(test_y, pred_y))
print(classification_report(test_y, pred_y, target_names=['Churn','NotChurn']))

ADBmodel = AdaBoostClassifier()
ADBmodel.fit(train_x,train_y)
pred_y = ADBmodel.predict(test_x)

print (metrics.accuracy_score(test_y, pred_y))
print(confusion_matrix(test_y, pred_y))
print(classification_report(test_y, pred_y, target_names=['Churn','NotChurn']))

clf = MLPClassifier()
clf.fit(train_x,train_y)
pred_y= clf.predict(test_x)

print (metrics.accuracy_score(test_y, pred_y))
print(confusion_matrix(test_y, pred_y))
print(classification_report(test_y, pred_y, target_names=['Churn','NotChurn']))

